{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neccessary Imports\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import graphviz \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing,tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.feature_selection import SelectFromModel,RFE\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from data_load import get_encoded_data,get_one_hot_encoded_data,get_clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Remaining After LE,SFM,LR ['hs_average', 'parent1_education', 'parent2_education', 'social_time', 'screen_time']\n",
      "Features Remaining After OHE,SFM,LR ['current_year_2', 'current_year_3', 'current_year_4', 'hs_average_90-94%', 'hs_average_95-100%', 'nationality_status_1st_Gen', 'nationality_status_2nd+_Gen', 'nationality_status_Internationl', 'parent1_education_High School', 'parent1_education_Post-Graduate (Masters +)', 'parent1_education_Undergraduate', 'parent2_education_College (Diploma)', 'social_time_Multiple Weekly', 'social_time_Rarely', 'class_attendance_Around half', 'screen_time_A significant amount', 'screen_time_Almost Always', 'sleep_time_Enough', 'sleep_time_Not Enough', 'excercise_time_0 - 1 day', 'excercise_time_2 - 3 days', 'school_work_time_I usually keep up', 'school_work_time_Only before a deliverable', 'coop_time_A lot less than school work', 'coop_time_Significant,but less than school', 'academic_priority_Yes']\n",
      "Features Remaining After LE,RFE,NB ['nationality_status', 'parent1_education', 'parent2_education', 'screen_time', 'coop_time']\n",
      "Features Remaining After LE,RFE,LR ['current_year_2', 'current_year_4', 'hs_average_90-94%', 'hs_average_95-100%', 'nationality_status_2nd+_Gen', 'parent1_education_High School', 'parent2_education_College (Diploma)', 'parent2_education_Undergraduate (Bachelors)', 'social_time_Rarely', 'screen_time_A significant amount', 'screen_time_Almost Always', 'sleep_time_Not Enough', 'excercise_time_0 - 1 day', 'school_work_time_I usually keep up', 'school_work_time_Only before a deliverable', 'coop_time_Significant,but less than school', 'academic_priority_Yes']\n",
      "[NB,Full Data,Label Encoded] Accuracy: 0.23 (+/- 0.84)\n",
      "[NB,Full Data,One Hot Encoded] Accuracy: 0.24 (+/- 0.86)\n",
      "[TREE,SFM,Label Encoded] Accuracy: 0.24 (+/- 0.86)\n",
      "[TREE,SFM,One Hot Encoded] Accuracy: 0.26 (+/- 0.88)\n",
      "[TREE,RFE,Label Encoded]Accuracy: 0.18 (+/- 0.76)\n",
      "[TREE,RFE,One Hot Encoded] Accuracy: 0.21 (+/- 0.81)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###========================= TREE Full Data Label Encoded Data =================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "# encoded_dict_list = get_encoded_data('data.csv')[1]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "# Leave one out validation\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  ent = tree.DecisionTreeClassifier()\n",
    "  model = ent.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(ent.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "TREE_FD_LE_mean = np.array(accuracy).mean()\n",
    "TREE_FD_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###==========================TREE Full Data One Hot Encoded  ===================\n",
    "\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  ent = tree.DecisionTreeClassifier()\n",
    "  model = ent.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(ent.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "TREE_FD_OHE_mean = np.array(accuracy).mean()\n",
    "TREE_FD_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============TREE SFM Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = tree.DecisionTreeClassifier().fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After LE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  ent = tree.DecisionTreeClassifier()\n",
    "  model = ent.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(ent.score(X_test, y_test))\n",
    "\n",
    "TREE_SFM_LE_mean = np.array(accuracy).mean()\n",
    "TREE_SFM_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============TREE SFM Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = tree.DecisionTreeClassifier().fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After OHE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  ent = tree.DecisionTreeClassifier()\n",
    "  model = ent.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(ent.score(X_test, y_test))\n",
    "\n",
    "TREE_SFM_OHE_mean = np.array(accuracy).mean()\n",
    "TREE_SFM_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============TREE RFE Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "RFEM = tree.DecisionTreeClassifier()\n",
    "rfe = RFE(RFEM, 5)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,NB\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  ent = tree.DecisionTreeClassifier()\n",
    "  model = ent.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(ent.score(X_test, y_test))\n",
    "\n",
    "TREE_RFE_LE_mean = np.array(accuracy).mean()\n",
    "TREE_RFE_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============TREE RFE Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "RFEM =tree.DecisionTreeClassifier()\n",
    "rfe = RFE(RFEM, 17)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,LR\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  ent = tree.DecisionTreeClassifier()\n",
    "  model = ent.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(ent.score(X_test, y_test))\n",
    "\n",
    "TREE_RFE_OHE_mean = np.array(accuracy).mean()\n",
    "TREE_RFE_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "#Printing Results\n",
    "print(\"[NB,Full Data,Label Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (TREE_FD_LE_mean, TREE_FD_LE_variance))\n",
    "print(\"[NB,Full Data,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (TREE_FD_OHE_mean, TREE_FD_OHE_variance))\n",
    "print(\"[TREE,SFM,Label Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (TREE_SFM_LE_mean, TREE_SFM_LE_variance))\n",
    "print(\"[TREE,SFM,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (TREE_SFM_OHE_mean, TREE_SFM_OHE_variance))\n",
    "print(\"[TREE,RFE,Label Encoded]Accuracy: %0.2f (+/- %0.2f)\" % (TREE_RFE_LE_mean, TREE_RFE_LE_variance))\n",
    "print(\"[TREE,RFE,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (TREE_RFE_OHE_mean, TREE_RFE_OHE_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN,Full Data,Label Encoded] Accuracy: 0.17 (+/- 0.76)\n",
      "[KNN,Full Data,One Hot Encoded] Accuracy: 0.21 (+/- 0.81)\n"
     ]
    }
   ],
   "source": [
    "##=============================== KNN Full Data Label Encoded  ============\n",
    "\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "# encoded_dict_list = get_encoded_data('data.csv')[1]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "  model = KNN.fit(X_train, y_train)\n",
    "  accuracy.append(KNN.score(X_test, y_test))\n",
    "\n",
    "KNN_FD_LE_mean = np.array(accuracy).mean()\n",
    "KNN_FD_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###============================== KNN Full Data One Hot Encoded  ==============\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "  model = KNN.fit(X_train, y_train)\n",
    "  accuracy.append(KNN.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "KNN_FD_OH_mean = np.array(accuracy).mean()\n",
    "KNN_FD_OH_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "#Printing Results\n",
    "print(\"[KNN,Full Data,Label Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (KNN_FD_LE_mean, KNN_FD_LE_variance))\n",
    "print(\"[KNN,Full Data,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (KNN_FD_OH_mean, KNN_FD_OH_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Remaining After LE,SFM,LR ['current_year', 'hs_average', 'class_attendance', 'academic_priority']\n",
      "Features Remaining After OHE,SFM,LR ['current_year_2', 'current_year_3', 'current_year_4', 'hs_average_80-84%', 'hs_average_95-100%', 'nationality_status_2nd+_Gen', 'parent1_education_College (Diploma)', 'parent1_education_High School', 'parent1_education_Post-Graduate', 'parent1_education_Undergraduate', 'parent2_education_College (Diploma)', 'parent2_education_Post-Graduate', 'parent2_education_Undergraduate', 'class_attendance_Almost all', 'screen_time_Almost never', 'sleep_time_Not Enough', 'school_work_time_Only before a deliverable', \"school_work_time_Very regularly, I'm always on top of my work\", 'coop_time_Almost none', 'coop_time_Same or More', 'academic_priority_No', 'academic_priority_Yes']\n",
      "Features Remaining After LE,RFE,LR ['current_year', 'hs_average', 'social_time', 'class_attendance', 'academic_priority']\n",
      "Features Remaining After LE,RFE,LR ['current_year_3', 'current_year_4', 'hs_average_80-84%', 'hs_average_95-100%', 'nationality_status_2nd+_Gen', 'parent1_education_College (Diploma)', 'parent1_education_Post-Graduate', 'parent1_education_Undergraduate (Bachelors)', 'parent2_education_Post-Graduate', 'class_attendance_Almost all', 'class_attendance_Around half', 'sleep_time_Enough', 'school_work_time_I usually keep up', 'school_work_time_Only before a deliverable', 'coop_time_Same or More', 'academic_priority_No', 'academic_priority_Yes']\n",
      "[LR,Full Data,Label Encoded]  Accuracy: 0.28 (+/- 0.90)\n",
      "[LR,Full Data,One Hot Encoded]  Accuracy: 0.24 (+/- 0.85)\n",
      "[LR,SFM,Label Encoded]Accuracy: 0.29 (+/- 0.91)\n",
      "[LR,SFM,One Hot Encoded]  Accuracy: 0.29 (+/- 0.91)\n",
      "[LR,RFE,Label Encoded]Accuracy: 0.33 (+/- 0.94)\n",
      "[LR,RFE,One Hot Encoded]  Accuracy: 0.28 (+/- 0.89)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###===========================LR Full Data Label Encoded Data =================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  LRR = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "  model = LRR.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(LRR.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "LR_FD_LE_mean = np.array(accuracy).mean()\n",
    "LR_FD_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===========================LR Full Data One Hot Encoded  ===================\n",
    "\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  LRR = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "  model = LRR.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(LRR.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "LR_FD_OH_mean = np.array(accuracy).mean()\n",
    "LR_FD_OH_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR SFM Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "#\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial').fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After LE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  LRR = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "  model = LRR.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(LRR.score(X_test, y_test))\n",
    "\n",
    "LR_SFM_LE_mean = np.array(accuracy).mean()\n",
    "LR_SFM_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR SFM Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial').fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After OHE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  LRR = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "  model = LRR.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(LRR.score(X_test, y_test))\n",
    "\n",
    "LR_SFM_OHE_mean = np.array(accuracy).mean()\n",
    "LR_SFM_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR RFE Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "LRR = LogisticRegression()\n",
    "rfe = RFE(LRR, 5)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,LR\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  LRR = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "  model = LRR.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(LRR.score(X_test, y_test))\n",
    "\n",
    "LR_RFE_LE_mean = np.array(accuracy).mean()\n",
    "LR_RFE_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR RFE Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "LRR = LogisticRegression()\n",
    "rfe = RFE(LRR, 17)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,LR\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  LRR = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "  model = LRR.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(LRR.score(X_test, y_test))\n",
    "\n",
    "LR_RFE_OHE_mean = np.array(accuracy).mean()\n",
    "LR_RFE_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "# Printing Results\n",
    "print(\"[LR,Full Data,Label Encoded]  Accuracy: %0.2f (+/- %0.2f)\" % (LR_FD_LE_mean, LR_FD_LE_variance))\n",
    "print(\"[LR,Full Data,One Hot Encoded]  Accuracy: %0.2f (+/- %0.2f)\" % (LR_FD_OH_mean, LR_FD_OH_variance))\n",
    "print(\"[LR,SFM,Label Encoded]Accuracy: %0.2f (+/- %0.2f)\" % (LR_SFM_LE_mean, LR_SFM_LE_variance))\n",
    "print(\"[LR,SFM,One Hot Encoded]  Accuracy: %0.2f (+/- %0.2f)\" % (LR_SFM_OHE_mean, LR_SFM_OHE_variance))\n",
    "print(\"[LR,RFE,Label Encoded]Accuracy: %0.2f (+/- %0.2f)\" % (LR_RFE_LE_mean, LR_RFE_LE_variance))\n",
    "print(\"[LR,RFE,One Hot Encoded]  Accuracy: %0.2f (+/- %0.2f)\" % (LR_RFE_OHE_mean, LR_RFE_OHE_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Remaining After LE,SFM,LR ['current_year', 'nationality_status', 'social_time', 'class_attendance', 'screen_time', 'sleep_time', 'excercise_time', 'school_work_time', 'academic_priority']\n",
      "Features Remaining After OHE,SFM,LR ['hs_average_80-84%', 'hs_average_85-89%', 'hs_average_<80%', 'nationality_status_Internationl', 'parent1_education_College', 'parent1_education_College (Diploma)', 'parent1_education_High School', 'parent1_education_Post-Graduate', 'parent1_education_Undergraduate', 'parent2_education_College', 'parent2_education_College (Diploma)', 'parent2_education_High School', 'parent2_education_Post-Graduate', 'parent2_education_Post-Graduate (Masters +)', 'parent2_education_Undergraduate', 'social_time_Rarely', 'class_attendance_Almost none', 'screen_time_Almost never', 'screen_time_Regularly', 'sleep_time_More than Enough', 'excercise_time_4+ days', \"school_work_time_Very regularly, I'm always on top of my work\", 'coop_time_Almost none', 'coop_time_Same or More']\n",
      "Features Remaining After LE,RFE,NB ['class_attendance', 'screen_time', 'excercise_time', 'school_work_time', 'academic_priority']\n",
      "Features Remaining After LE,RFE,LR ['hs_average_80-84%', 'hs_average_<80%', 'parent1_education_College', 'parent1_education_College (Diploma)', 'parent1_education_Post-Graduate', 'parent1_education_Undergraduate', 'parent2_education_College', 'parent2_education_College (Diploma)', 'parent2_education_Post-Graduate', 'parent2_education_Undergraduate', 'class_attendance_Almost none', 'screen_time_Almost never', 'screen_time_Regularly', 'sleep_time_More than Enough', 'excercise_time_4+ days', 'coop_time_Almost none', 'coop_time_Same or More']\n",
      "[NB,Full Data,Label Encoded]  Accuracy: 0.28 (+/- 0.89)\n",
      "[NB,Full Data,One Hot Encoded] Accuracy: 0.23 (+/- 0.84)\n",
      "[NB,SFM,Label Encoded] Accuracy: 0.29 (+/- 0.90)\n",
      "[NB,SFM,One Hot Encoded] Accuracy: 0.23 (+/- 0.84)\n",
      "[NB,RFE,Label Encoded]Accuracy: 0.31 (+/- 0.92)\n",
      "[NB,RFE,One Hot Encoded] Accuracy: 0.20 (+/- 0.80)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###=========================== NB Full Data Label Encoded Data =================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  mnb = MultinomialNB()\n",
    "  model = mnb.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(mnb.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "NB_FD_LE_mean = np.array(accuracy).mean()\n",
    "NB_FD_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===========================NB Full Data One Hot Encoded  ===================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  mnb = MultinomialNB()\n",
    "  model = mnb.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(mnb.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "NB_FD_OH_mean = np.array(accuracy).mean()\n",
    "NB_FD_OH_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR SFM Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "#\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = MultinomialNB().fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After LE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  mnb = MultinomialNB()\n",
    "  model = mnb.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(mnb.score(X_test, y_test))\n",
    "\n",
    "NB_SFM_LE_mean = np.array(accuracy).mean()\n",
    "NB_SFM_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR SFM Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = MultinomialNB().fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After OHE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  mnb = MultinomialNB()\n",
    "  model = mnb.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(mnb.score(X_test, y_test))\n",
    "\n",
    "NB_SFM_OHE_mean = np.array(accuracy).mean()\n",
    "NB_SFM_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============LR RFE Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "RFEM = MultinomialNB()\n",
    "rfe = RFE(RFEM, 5)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,NB\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  mnb = MultinomialNB()\n",
    "  model = mnb.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(mnb.score(X_test, y_test))\n",
    "\n",
    "NB_RFE_LE_mean = np.array(accuracy).mean()\n",
    "NB_RFE_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "\n",
    "###===============NB RFE Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "RFEM = MultinomialNB()\n",
    "rfe = RFE(RFEM, 17)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,LR\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) #\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "\n",
    "  mnb = MultinomialNB()\n",
    "  model = mnb.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(mnb.score(X_test, y_test))\n",
    "\n",
    "NB_RFE_OHE_mean = np.array(accuracy).mean()\n",
    "NB_RFE_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "#Printing Results\n",
    "print(\"[NB,Full Data,Label Encoded]  Accuracy: %0.2f (+/- %0.2f)\" % (NB_FD_LE_mean, NB_FD_LE_variance))\n",
    "print(\"[NB,Full Data,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (NB_FD_OH_mean, NB_FD_OH_variance))\n",
    "print(\"[NB,SFM,Label Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (NB_SFM_LE_mean, NB_SFM_LE_variance))\n",
    "print(\"[NB,SFM,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (NB_SFM_OHE_mean, NB_SFM_OHE_variance))\n",
    "print(\"[NB,RFE,Label Encoded]Accuracy: %0.2f (+/- %0.2f)\" % (NB_RFE_LE_mean, NB_RFE_LE_variance))\n",
    "print(\"[NB,RFE,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (NB_RFE_OHE_mean, NB_RFE_OHE_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Remaining After LE,SFM,LR ['current_year', 'hs_average', 'social_time', 'sleep_time', 'academic_priority']\n",
      "Features Remaining After OHE,SFM,LR ['hs_average_80-84%', 'hs_average_95-100%', 'hs_average_<80%', 'nationality_status_Internationl', 'parent1_education_College', 'parent1_education_College (Diploma)', 'parent1_education_High School', 'parent1_education_Post-Graduate', 'parent1_education_Post-Graduate (Masters +)', 'parent1_education_Undergraduate', 'parent1_education_Undergraduate (Bachelors)', 'parent2_education_College', 'parent2_education_College (Diploma)', 'parent2_education_High School', 'parent2_education_Post-Graduate', 'parent2_education_Undergraduate', 'screen_time_Almost never', \"school_work_time_Very regularly, I'm always on top of my work\", 'coop_time_Almost none', 'coop_time_Same or More', 'academic_priority_No', 'academic_priority_Yes']\n",
      "Features Remaining After LE,RFE,NB ['current_year', 'hs_average', 'social_time', 'school_work_time', 'academic_priority']\n",
      "Features Remaining After LE,RFE,LR ['hs_average_80-84%', 'hs_average_<80%', 'nationality_status_2nd+_Gen', 'parent1_education_College', 'parent1_education_College (Diploma)', 'parent1_education_High School', 'parent1_education_Post-Graduate', 'parent1_education_Undergraduate', 'parent2_education_College', 'parent2_education_College (Diploma)', 'parent2_education_Post-Graduate', 'parent2_education_Undergraduate', 'screen_time_Almost never', 'sleep_time_Enough', 'coop_time_Same or More', 'academic_priority_No', 'academic_priority_Yes']\n",
      "[SVM,Full Data,Label Encoded]  Accuracy: 0.25 (+/- 0.87)\n",
      "[SVM,Full Data,One Hot Encoded] Accuracy: 0.24 (+/- 0.85)\n",
      "[SVM,SFM,Label Encoded] Accuracy: 0.29 (+/- 0.90)\n",
      "[SVM,SFM,One Hot Encoded] Accuracy: 0.26 (+/- 0.87)\n",
      "[SVM,RFE,Label Encoded]Accuracy: 0.32 (+/- 0.93)\n",
      "[SVM,RFE,One Hot Encoded] Accuracy: 0.28 (+/- 0.90)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###=========================== SVM Full Data Label Encoded Data ==============\n",
    "\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "# encoded_dict_list = get_encoded_data('data.csv')[1]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  SVM = LinearSVC()\n",
    "  model = SVM.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(SVM.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "SVM_FD_LE_mean = np.array(accuracy).mean()\n",
    "SVM_FD_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###======================== SVM Full Data One Hot Encoded  ===================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df) # convert dataframe into np array\n",
    "y = np.array(y_df) # convert dataframe into np array\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  SVM = LinearSVC()\n",
    "  model = SVM.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(SVM.score(X_test, y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "SVM_FD_OHE_mean = np.array(accuracy).mean()\n",
    "SVM_FD_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "# ###===============SVM SFM Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "#\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = LinearSVC().fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After LE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  SVM = LinearSVC()\n",
    "  model = SVM.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(SVM.score(X_test, y_test))\n",
    "\n",
    "SVM_SFM_LE_mean = np.array(accuracy).mean()\n",
    "SVM_SFM_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "\n",
    "###===============SVM SFM Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "SFM = LinearSVC().fit(x_df,y_df)\n",
    "\n",
    "modelNew = SelectFromModel(SFM, prefit = True)\n",
    "fs_x_df = modelNew.transform(x_df)\n",
    "feature_idx = modelNew.get_support()\n",
    "feature_name = x_df.columns[feature_idx]\n",
    "print(\"Features Remaining After OHE,SFM,LR\",list(feature_name))\n",
    "X = np.array(fs_x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  SVM = LinearSVC()\n",
    "  model = SVM.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(SVM.score(X_test, y_test))\n",
    "\n",
    "SVM_SFM_OHE_mean = np.array(accuracy).mean()\n",
    "SVM_SFM_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============SVM RFE Data Label Encoded Data  =======================\n",
    "df = get_encoded_data('data.csv')[0]\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "RFEM = LinearSVC()\n",
    "rfe = RFE(RFEM, 5)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,NB\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  SVM = LinearSVC()\n",
    "  model = SVM.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(SVM.score(X_test, y_test))\n",
    "\n",
    "SVM_RFE_LE_mean = np.array(accuracy).mean()\n",
    "SVM_RFE_LE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "###===============SVM RFE Data One Hot Encoded Data  =======================\n",
    "df = get_one_hot_encoded_data('data.csv')\n",
    "\n",
    "x_df = df.drop(axis=1,columns=[\"current_average\"])\n",
    "y_df = df[\"current_average\"]\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "RFEM = LinearSVC()\n",
    "rfe = RFE(RFEM, 17)\n",
    "fit = rfe.fit(X, y)\n",
    "featureidx = fit.get_support()\n",
    "feature_names = list(x_df.columns[featureidx])\n",
    "x_df = x_df[feature_names]\n",
    "\n",
    "print(\"Features Remaining After LE,RFE,LR\",list(feature_names))\n",
    "\n",
    "X = np.array(x_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "LeaveOneOut()\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "  X_train, X_test = pd.DataFrame(X[train_index]), pd.DataFrame(X[test_index]) # use this for training the model\n",
    "  y_train, y_test = y[train_index].ravel(), y[test_index].ravel() # use this for testing the model\n",
    "\n",
    "  SVM = LinearSVC()\n",
    "  model = SVM.fit(X_train, y_train) # fit the model using training data\n",
    "  accuracy.append(SVM.score(X_test, y_test))\n",
    "\n",
    "SVM_RFE_OHE_mean = np.array(accuracy).mean()\n",
    "SVM_RFE_OHE_variance = np.array(accuracy).std() * 2\n",
    "\n",
    "#Printing Results\n",
    "print(\"[SVM,Full Data,Label Encoded]  Accuracy: %0.2f (+/- %0.2f)\" % (SVM_FD_LE_mean, SVM_FD_LE_variance))\n",
    "print(\"[SVM,Full Data,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (SVM_FD_OHE_mean, SVM_FD_OHE_variance))\n",
    "print(\"[SVM,SFM,Label Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (SVM_SFM_LE_mean, SVM_SFM_LE_variance))\n",
    "print(\"[SVM,SFM,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (SVM_SFM_OHE_mean, SVM_SFM_OHE_variance))\n",
    "print(\"[SVM,RFE,Label Encoded]Accuracy: %0.2f (+/- %0.2f)\" % (SVM_RFE_LE_mean, SVM_RFE_LE_variance))\n",
    "print(\"[SVM,RFE,One Hot Encoded] Accuracy: %0.2f (+/- %0.2f)\" % (SVM_RFE_OHE_mean, SVM_RFE_OHE_variance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
